{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import random\n",
    "from collections import defaultdict\n",
    "from scipy.io import wavfile\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_speaker_roots_in_data_path (datapath ='accents'):\n",
    "    speaker_list = []\n",
    "    accent_subfolders = [f.path for f in os.scandir(datapath) if  f.is_dir()]\n",
    "    for accent in accent_subfolders:\n",
    "        for gender in ['female','male']:\n",
    "            speaker_folders = os.listdir(os.path.join(accent,gender))\n",
    "            for speaker in speaker_folders:\n",
    "                if not speaker.startswith('.'):\n",
    "                    speaker_list.append(os.path.join(accent,gender,speaker))\n",
    "    return speaker_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wav_files_in_path(datapath):\n",
    "    files = os.listdir(datapath)\n",
    "    files_wav = [i for i in files if i.endswith('.wav')]\n",
    "    return files_wav"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(data, train_ratio, val_ratio, seed=42):\n",
    "    random.seed(seed)\n",
    "    speaker_data = defaultdict(list)\n",
    "    for item in data:\n",
    "        speaker_id = item.split('_')[0]\n",
    "        speaker_data[speaker_id].append(item)\n",
    "    \n",
    "    train_data = []\n",
    "    val_data = []\n",
    "    test_data = []\n",
    "    \n",
    "    for speaker_id, speaker_utterances in speaker_data.items():\n",
    "        n_utterances = len(speaker_utterances)\n",
    "        n_train = int(n_utterances * train_ratio)\n",
    "        n_val = int(n_utterances * val_ratio)\n",
    "        \n",
    "        random.shuffle(speaker_utterances)\n",
    "        train_data.extend(speaker_utterances[:n_train])\n",
    "        val_data.extend(speaker_utterances[n_train:n_train+n_val])\n",
    "        test_data.extend(speaker_utterances[n_train+n_val:])\n",
    "    \n",
    "    return train_data, val_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def segment_audio(filepath, chunk_length, sr):\n",
    "    rate, audio = wavfile.read(filepath)\n",
    "    chunk_samples = sr * chunk_length\n",
    "    chunk_stride = chunk_samples // 2\n",
    "    audio_length = len(audio)\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < audio_length:\n",
    "        end = start + chunk_samples\n",
    "        if end >= audio_length:\n",
    "            end = audio_length\n",
    "        chunk = audio[start:end]\n",
    "        chunks.append(chunk)\n",
    "        start += chunk_stride\n",
    "    return chunks\n",
    "\n",
    "def segment_audios(wav_files, datapath, chunk_length, sr):\n",
    "    all_chunks = []\n",
    "    for file in wav_files:\n",
    "        file_path = os.path.join(datapath, file)\n",
    "        chunks = segment_audio(file_path, chunk_length, sr)\n",
    "        all_chunks.extend(chunks)\n",
    "    return all_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #first root path of speaker_list is passed, chunks of 3 seconds \n",
    "# all_chunks = segment_audios(wav_files, speaker_list[0], 3, 16000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Reshape, Conv1D, MaxPooling1D, Flatten, Dense, LSTM\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "def create_model(input_shape, num_classes):\n",
    "    inputs = Input(shape=input_shape)\n",
    "    x = Reshape((input_shape[0], input_shape[1], 1))(inputs)\n",
    "    x = Conv1D(filters=32, kernel_size=3, activation='relu')(x)\n",
    "    x = MaxPooling1D(pool_size=2)(x)\n",
    "    x = Conv1D(filters=64, kernel_size=3, activation='relu')(x)\n",
    "    x = MaxPooling1D(pool_size=2)(x)\n",
    "    x = Conv1D(filters=128, kernel_size=3, activation='relu')(x)\n",
    "    x = MaxPooling1D(pool_size=2)(x)\n",
    "    x = LSTM(units=128, return_sequences=False)(x)\n",
    "    x = Dense(units=64, activation='relu')(x)\n",
    "    outputs = Dense(units=num_classes, activation='softmax')(x)\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-23eff9d0f619>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[1;31m#num_classes = len(accent_subfolders)  # number of accent subfolders\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 60\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     61\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-7-1259a484ad1e>\u001b[0m in \u001b[0;36mcreate_model\u001b[1;34m(input_shape, num_classes)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mcreate_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0minputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mInput\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mReshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m     \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mConv1D\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilters\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkernel_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'relu'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMaxPooling1D\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpool_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: tuple index out of range"
     ]
    }
   ],
   "source": [
    "# Collect all speaker paths\n",
    "speaker_list = get_speaker_roots_in_data_path()\n",
    "\n",
    "# Get all wav files\n",
    "wav_files = []\n",
    "for currPath in speaker_list:\n",
    "    curr_wavFiles = get_wav_files_in_path(currPath)\n",
    "    for i in curr_wavFiles:\n",
    "        wav_files.append(i)\n",
    "\n",
    "# Split data into training, validation, and test sets\n",
    "train_data, val_data, test_data = split_data(wav_files, 0.6, 0.2)\n",
    "\n",
    "# Segment the audio files into chunks\n",
    "train_chunks = segment_audios(train_data, speaker_list[0], 3, 16000)\n",
    "val_chunks = segment_audios(val_data, speaker_list[0], 3, 16000)\n",
    "\n",
    "max_len = max([len(chunk) for chunk in train_chunks])\n",
    "for i, chunk in enumerate(train_chunks):\n",
    "    if len(chunk) < max_len:\n",
    "        padding = np.zeros(max_len - len(chunk), dtype=np.int16)\n",
    "        train_chunks[i] = np.concatenate((chunk, padding))\n",
    "    elif len(chunk) > max_len:\n",
    "        train_chunks[i] = chunk[:max_len]\n",
    "        \n",
    "max_len = max([len(chunk) for chunk in val_chunks])\n",
    "for i, chunk in enumerate(val_chunks):\n",
    "    if len(chunk) < max_len:\n",
    "        padding = np.zeros(max_len - len(chunk), dtype=np.int16)\n",
    "        val_chunks[i] = np.concatenate((chunk, padding))\n",
    "    elif len(chunk) > max_len:\n",
    "        val_chunks[i] = chunk[:max_len]\n",
    "\n",
    "# Convert the chunks into appropriate input data and labels\n",
    "x_train = np.stack(train_chunks, axis=0)\n",
    "x_val = np.stack(val_chunks, axis=0)\n",
    "# x_train = np.array(train_chunks) # replace with appropriate input data, potentially using train_chunks\n",
    "# y_train = np.array(train_data) # replace with appropriate labels, potentially using train_data\n",
    "# x_val = np.array(val_chunks) # replace with appropriate input data, potentially using val_chunks\n",
    "# y_val = np.array(val_data) # replace with appropriate labels, potentially using val_data\n",
    "\n",
    "#Create labels for each accent\n",
    "accent_subfolders = [f.path for f in os.scandir('accents') if f.is_dir()]\n",
    "num_classes = len(accent_subfolders)\n",
    "labels_dict = {}\n",
    "for i, folder in enumerate(accent_subfolders):\n",
    "    labels_dict[folder] = i\n",
    "\n",
    "#Create labels for each audio chunk\n",
    "y_train = [labels_dict[os.path.dirname(file_path)] if os.path.dirname(file_path) in labels_dict else -1 for file_path in train_data]\n",
    "y_val = [labels_dict[os.path.dirname(file_path)] if os.path.dirname(file_path) in labels_dict else -1 for file_path in val_data]\n",
    "\n",
    "#One-hot encode the labels\n",
    "y_train = to_categorical(y_train, num_classes=num_classes)\n",
    "y_val = to_categorical(y_val, num_classes=num_classes)\n",
    "\n",
    "input_shape = (16000 * 3,)  # Based on the segment length of 3 seconds and the sample rate of 16000\n",
    "#num_classes = len(accent_subfolders)  # number of accent subfolders\n",
    "\n",
    "model = create_model(input_shape, num_classes)\n",
    "model.fit(x_train, y_train, epochs=10, batch_size=32, validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
